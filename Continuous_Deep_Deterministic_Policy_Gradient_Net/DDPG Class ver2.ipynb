{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Deterministic Policy Gradient\n",
    "Implementation followed: Continuous control with deep reinforcement learning (arXiv:1509.02971v5)\n",
    "- Memory Relay\n",
    "- A3C\n",
    "- Trained with a target net\n",
    "- Initial exploration policy is quite important to warm up the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-26T20:05:05.197940\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now().isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Actor(object):\n",
    "    def __init__(self, n_observation, n_action, name='actor_net'):\n",
    "        self.n_observation = n_observation\n",
    "        self.n_action = n_action\n",
    "        self.name = name\n",
    "        self.sess = None\n",
    "        self.build_model()\n",
    "        self.build_train()\n",
    "        \n",
    "    def build_model(self):\n",
    "        activation = tf.nn.elu\n",
    "        kernel_initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "        kernel_regularizer = tf.contrib.layers.l2_regularizer(0.1)\n",
    "        default_dense = partial(tf.layers.dense,\\\n",
    "                                activation=activation,\\\n",
    "                                kernel_initializer=kernel_initializer,\\\n",
    "                                kernel_regularizer=kernel_regularizer)\n",
    "        with tf.variable_scope(self.name) as scope:\n",
    "            observation = tf.placeholder(tf.float32,shape=[None,self.n_observation])\n",
    "            hid1 = default_dense(observation,32)\n",
    "            hid2 = default_dense(hid1,64)\n",
    "            action = default_dense(hid2,self.n_action,activation=tf.nn.tanh,use_bias=False)\n",
    "            trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=self.name)\n",
    "        self.observation,self.action,self.trainable_vars = observation,action,trainable_vars\n",
    "        \n",
    "    def build_train(self,learning_rate = 0.0001):\n",
    "        with tf.variable_scope(self.name) as scope:\n",
    "            action_grads = tf.placeholder(tf.float32,[None,self.n_action])\n",
    "            var_grads = tf.gradients(self.action,self.trainable_vars,-action_grads)\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate).apply_gradients(zip(var_grads,self.trainable_vars))\n",
    "        self.action_grads,self.train_op = action_grads,train_op\n",
    "        \n",
    "    def predict_action(self,obs_batch):\n",
    "        return self.action.eval(session=self.sess,feed_dict={self.observation:obs_batch})\n",
    "\n",
    "    def train(self,obs_batch,action_grads):\n",
    "        batch_size = len(action_grads)\n",
    "        self.train_op.run(session=self.sess,feed_dict={self.observation:obs_batch,self.action_grads:action_grads/batch_size})\n",
    "        \n",
    "    def set_session(self,sess):\n",
    "        self.sess = sess\n",
    "        \n",
    "    def get_trainable_dict(self):\n",
    "        return {var.name[len(self.name):]: var for var in self.trainable_vars}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Critic(object):\n",
    "    def __init__(self, n_observation, n_action, name='critic_net'):\n",
    "        self.n_observation = n_observation\n",
    "        self.n_action = n_action\n",
    "        self.name = name\n",
    "        self.sess = None\n",
    "        self.build_model()\n",
    "        self.build_train()\n",
    "        \n",
    "    def build_model(self):\n",
    "        activation = tf.nn.elu\n",
    "        kernel_initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "        kernel_regularizer = tf.contrib.layers.l2_regularizer(0.1)\n",
    "        default_dense = partial(tf.layers.dense,\\\n",
    "                                activation=activation,\\\n",
    "                                kernel_initializer=kernel_initializer,\\\n",
    "                                kernel_regularizer=kernel_regularizer)\n",
    "        with tf.variable_scope(self.name) as scope:\n",
    "            observation = tf.placeholder(tf.float32,shape=[None,self.n_observation])\n",
    "            action = tf.placeholder(tf.float32,shape=[None,self.n_action])\n",
    "            hid1 = default_dense(observation,32)\n",
    "            hid2 = default_dense(action,32)\n",
    "            hid3 = tf.concat([hid1,hid2],axis=1)\n",
    "            hid4 = default_dense(hid3,128)\n",
    "            Q = default_dense(hid4,1, activation=None)\n",
    "            trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=self.name)\n",
    "        self.observation,self.action,self.Q,self.trainable_vars= observation,action,Q,trainable_vars\n",
    "    \n",
    "    def build_train(self,learning_rate=0.001):\n",
    "        with tf.variable_scope(self.name) as scope:\n",
    "            Qexpected = tf.placeholder(tf.float32,shape=[None,1])\n",
    "            loss = tf.losses.mean_squared_error(Qexpected,self.Q)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            train_op = optimizer.minimize(loss)\n",
    "        self.Qexpected,self.train_op = Qexpected,train_op\n",
    "        self.action_grads = tf.gradients(self.Q,self.action)[0]\n",
    "    \n",
    "    def predict_Q(self,obs_batch,action_batch):\n",
    "        return self.Q.eval(session=self.sess,\\\n",
    "                           feed_dict={self.observation:obs_batch,self.action:action_batch})\n",
    "    \n",
    "    def compute_action_grads(self,obs_batch,action_batch):\n",
    "        return self.action_grads.eval(session=self.sess,\\\n",
    "                               feed_dict={self.observation:obs_batch,self.action:action_batch})\n",
    "    def train(self,obs_batch,action_batch,Qexpected_batch):\n",
    "        self.train_op.run(session=self.sess,\\\n",
    "                          feed_dict={self.observation:obs_batch,self.action:action_batch,self.Qexpected:Qexpected_batch})\n",
    "    \n",
    "    def set_session(self,sess):\n",
    "        self.sess = sess\n",
    "        \n",
    "    def get_trainable_dict(self):\n",
    "        return {var.name[len(self.name):]: var for var in self.trainable_vars}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AsyncNets(object):\n",
    "    def __init__(self,class_name):\n",
    "        class_ = eval(class_name)\n",
    "        self.net = class_(2,1,name=class_name)\n",
    "        self.target_net = class_(2,1,name='{}_target'.format(class_name))\n",
    "        self.TAU = tf.placeholder(tf.float32,shape=None)\n",
    "        self.sess = None\n",
    "        self.__build_async_assign()\n",
    "    \n",
    "    def __build_async_assign(self):\n",
    "        net_dict = self.net.get_trainable_dict()\n",
    "        target_net_dict = self.target_net.get_trainable_dict()\n",
    "        keys = net_dict.keys()\n",
    "        async_update_op = [target_net_dict[key].assign((1-self.TAU)*target_net_dict[key]+self.TAU*net_dict[key]) \\\n",
    "                           for key in keys]\n",
    "        self.async_update_op = async_update_op\n",
    "    \n",
    "    def async_update(self,tau=0.01):\n",
    "        self.sess.run(self.async_update_op,feed_dict={self.TAU:tau})\n",
    "    \n",
    "    def set_session(self,sess):\n",
    "        self.sess = sess\n",
    "        self.net.set_session(sess)\n",
    "        self.target_net.set_session(sess)\n",
    "    \n",
    "    def get_subnets(self):\n",
    "        return self.net, self.target_net\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory(object):\n",
    "    def __init__(self,memory_size=10000):\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.memory_size = memory_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def append(self,item):\n",
    "        self.memory.append(item)\n",
    "        \n",
    "    def sample_batch(self,batch_size=256):\n",
    "        idx = np.random.permutation(len(self.memory))[:batch_size]\n",
    "        return [self.memory[i] for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def UONoise():\n",
    "    theta = 0.15\n",
    "    sigma = 0.2\n",
    "    state = 0\n",
    "    while True:\n",
    "        yield state\n",
    "        state += -theta*state+sigma*np.random.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-26 19:41:54,989] Making new env: MountainCarContinuous-v0\n",
      "[2017-08-26 19:41:55,000] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/winter/Google Drive/handson-ml/tmp')\n",
      "[2017-08-26 19:41:55,008] Clearing 12 monitor files from previous run (because force=True was provided)\n",
      "[2017-08-26 19:41:55,010] Starting new video recorder writing to /Users/winter/Google Drive/handson-ml/tmp/openaigym.video.2.955.video000000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 993, ep 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-26 19:42:12,100] Starting new video recorder writing to /Users/winter/Google Drive/handson-ml/tmp/openaigym.video.2.955.video000001.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 998, ep 0, score -16.821533, steps 999\n",
      "iter 1575, ep 1, score 90.513038, steps 577\n",
      "iter 2574, ep 2, score -13.971783, steps 999\n",
      "iter 3573, ep 3, score -12.553009, steps 999\n",
      "iter 4338, ep 4, score 88.878217, steps 765\n",
      "iter 5337, ep 5, score -14.499419, steps 999\n",
      "iter 6048, ep 6, score 91.099272, steps 711\n",
      "iter 6676, ep 7"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-26 19:42:55,373] Starting new video recorder writing to /Users/winter/Google Drive/handson-ml/tmp/openaigym.video.2.955.video000008.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", score 90.996675, steps 628\n",
      "iter 7675, ep 8, score -10.819755, steps 999\n",
      "iter 8674, ep 9, score -11.558511, steps 999\n",
      "iter 8979, ep 10, score 94.314570, steps 305\n",
      "iter 9978, ep 11, score -10.546130, steps 999\n",
      "iter 10977, ep 12, score -12.766908, steps 999\n",
      "iter 11976, ep 13, score -10.189042, steps 999\n",
      "iter 12975, ep 14, score -11.460311, steps 999\n",
      "iter 13974, ep 15, score -13.411151, steps 999\n",
      "iter 14973, ep 16, score -14.304451, steps 999\n",
      "iter 15676, ep 17, score 88.887782, steps 703\n",
      "iter 16242, ep 18, score 92.691063, steps 566\n",
      "iter 17241, ep 19, score -11.693453, steps 999\n",
      "iter 18023, ep 20, score 90.685539, steps 782\n",
      "iter 19022, ep 21, score -13.186908, steps 999\n",
      "iter 19463, ep 22, score 94.026987, steps 441\n",
      "iter 20462, ep 23, score -13.140392, steps 999\n",
      "iter 21063, ep 24, score 90.732156, steps 601\n",
      "iter 22062, ep 25, score -15.018492, steps 999\n",
      "iter 22621, ep 26"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-26 19:44:53,473] Starting new video recorder writing to /Users/winter/Google Drive/handson-ml/tmp/openaigym.video.2.955.video000027.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 22644, ep 26, score 93.464829, steps 582\n",
      "iter 23408, ep 27, score 89.554092, steps 764\n",
      "iter 24091, ep 28, score 87.892311, steps 683\n",
      "iter 25005, ep 29, score 85.951233, steps 914\n",
      "iter 26004, ep 30, score -12.744702, steps 999\n",
      "iter 26531, ep 31, score 94.225969, steps 527\n",
      "iter 27530, ep 32, score -16.797004, steps 999\n",
      "iter 28219, ep 33, score 89.064114, steps 689\n",
      "iter 28892, ep 34, score 85.746313, steps 673\n",
      "iter 29891, ep 35, score -17.804687, steps 999\n",
      "iter 30890, ep 36, score -20.038071, steps 999\n",
      "iter 31889, ep 37, score -21.388418, steps 999\n",
      "iter 32613, ep 38, score 86.763517, steps 724\n",
      "iter 32895, ep 39, score 94.907800, steps 282\n",
      "iter 33183, ep 40, score 96.955224, steps 288\n",
      "iter 33407, ep 41, score 96.408442, steps 224\n",
      "iter 33651, ep 42, score 95.072191, steps 244\n",
      "iter 33817, ep 43, score 97.042655, steps 166\n",
      "iter 34050, ep 44, score 95.092518, steps 233\n",
      "iter 34209, ep 45, score 97.400811, steps 159\n",
      "iter 34365, ep 46, score 97.076128, steps 156\n",
      "iter 34601, ep 47, score 94.900098, steps 236\n",
      "iter 34757, ep 48, score 96.290122, steps 156\n",
      "iter 35005, ep 49, score 95.847698, steps 248\n",
      "iter 35173, ep 50, score 96.491020, steps 168\n",
      "iter 35333, ep 51, score 95.923480, steps 160\n",
      "iter 35488, ep 52, score 95.869699, steps 155\n",
      "iter 35645, ep 53, score 96.014779, steps 157\n",
      "iter 35816, ep 54, score 95.952439, steps 171\n",
      "iter 35971, ep 55, score 95.244905, steps 155\n",
      "iter 36129, ep 56, score 95.436702, steps 158\n",
      "iter 36287, ep 57, score 94.857622, steps 158\n",
      "iter 36445, ep 58, score 94.044361, steps 158\n",
      "iter 36586, ep 59, score 95.303828, steps 141\n",
      "iter 36675, ep 60, score 96.390997, steps 89\n",
      "iter 36837, ep 61, score 94.398890, steps 162\n",
      "iter 36992, ep 62, score 93.246401, steps 155\n",
      "iter 37141, ep 63"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-26 19:46:36,338] Starting new video recorder writing to /Users/winter/Google Drive/handson-ml/tmp/openaigym.video.2.955.video000064.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 37144, ep 63, score 93.996951, steps 152\n",
      "iter 37291, ep 64, score 94.829838, steps 147\n",
      "iter 37467, ep 65, score 93.702264, steps 176\n",
      "iter 37644, ep 66, score 92.224951, steps 177\n",
      "iter 37794, ep 67, score 93.515279, steps 150\n",
      "iter 37950, ep 68, score 93.164297, steps 156\n",
      "iter 38083, ep 69, score 94.774690, steps 133\n",
      "iter 38235, ep 70, score 93.599692, steps 152\n",
      "iter 38354, ep 71, score 95.467911, steps 119\n",
      "iter 38484, ep 72, score 94.388090, steps 130\n",
      "iter 38558, ep 73, score 95.755785, steps 74\n",
      "iter 38706, ep 74, score 92.254909, steps 148\n",
      "iter 38783, ep 75, score 95.610128, steps 77\n",
      "iter 38864, ep 76, score 95.461576, steps 81\n",
      "iter 38967, ep 77, score 94.318447, steps 103\n",
      "iter 39102, ep 78, score 92.680197, steps 135\n",
      "iter 39179, ep 79, score 95.486308, steps 77\n",
      "iter 39318, ep 80, score 92.853924, steps 139\n",
      "iter 39394, ep 81, score 95.659714, steps 76\n",
      "iter 39465, ep 82, score 95.117314, steps 71\n",
      "iter 39547, ep 83, score 94.727829, steps 82\n",
      "iter 39629, ep 84, score 94.514347, steps 82\n",
      "iter 39704, ep 85, score 94.652798, steps 75\n",
      "iter 39777, ep 86, score 94.445660, steps 73\n",
      "iter 39858, ep 87, score 94.239863, steps 81\n",
      "iter 39929, ep 88, score 94.312136, steps 71\n",
      "iter 40001, ep 89, score 94.509007, steps 72\n",
      "iter 40072, ep 90, score 94.666801, steps 71\n",
      "iter 40145, ep 91, score 94.389765, steps 73\n",
      "iter 40214, ep 92, score 94.419837, steps 69\n",
      "iter 40282, ep 93, score 94.341991, steps 68\n",
      "iter 40355, ep 94, score 94.098858, steps 73\n",
      "iter 40423, ep 95, score 94.070725, steps 68\n",
      "iter 40491, ep 96, score 93.962244, steps 68\n",
      "iter 40558, ep 97, score 93.858663, steps 67\n",
      "iter 40627, ep 98, score 93.886947, steps 69\n",
      "iter 40694, ep 99, score 93.802138, steps 67\n",
      "iter 40760, ep 100, score 93.595257, steps 66\n",
      "iter 40857, ep 101, score 91.245881, steps 97\n",
      "iter 40923, ep 102, score 93.663442, steps 66\n",
      "iter 40989, ep 103, score 93.636351, steps 66\n",
      "iter 41055, ep 104, score 93.594846, steps 66\n",
      "iter 41149, ep 105, score 91.236285, steps 94\n",
      "iter 41215, ep 106, score 93.607248, steps 66\n",
      "iter 41280, ep 107, score 93.701786, steps 65\n",
      "iter 41345, ep 108, score 93.706605, steps 65\n",
      "iter 41411, ep 109, score 93.658946, steps 66\n",
      "iter 41505, ep 110, score 91.134111, steps 94\n",
      "iter 41572, ep 111, score 93.676209, steps 67\n",
      "iter 41667, ep 112, score 91.063218, steps 95\n",
      "iter 41733, ep 113, score 93.674531, steps 66\n",
      "iter 41827, ep 114, score 91.197605, steps 94\n",
      "iter 41893, ep 115, score 93.654271, steps 66\n",
      "iter 41958, ep 116, score 93.711481, steps 65\n",
      "iter 42024, ep 117, score 93.648411, steps 66\n",
      "iter 42089, ep 118, score 93.672089, steps 65\n",
      "iter 42159, ep 119, score 93.721641, steps 70\n",
      "iter 42225, ep 120, score 93.626316, steps 66\n",
      "iter 42318, ep 121, score 91.763871, steps 93\n",
      "iter 42383, ep 122, score 93.714648, steps 65\n",
      "iter 42478, ep 123, score 90.994213, steps 95\n",
      "iter 42560, ep 124"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-26 19:47:15,862] Starting new video recorder writing to /Users/winter/Google Drive/handson-ml/tmp/openaigym.video.2.955.video000125.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 42572, ep 124, score 91.147539, steps 94\n",
      "iter 42668, ep 125, score 90.983828, steps 96\n",
      "iter 42761, ep 126, score 91.405670, steps 93\n",
      "iter 42826, ep 127, score 93.698148, steps 65\n",
      "iter 42897, ep 128, score 93.416037, steps 71\n",
      "iter 42963, ep 129, score 93.638845, steps 66\n",
      "iter 43028, ep 130, score 93.683336, steps 65\n",
      "iter 43094, ep 131, score 93.612474, steps 66\n",
      "iter 43175, ep 132, score 93.375587, steps 81\n",
      "iter 43243, ep 133, score 93.515115, steps 68\n",
      "iter 43309, ep 134, score 93.638845, steps 66\n",
      "iter 43380, ep 135, score 93.605524, steps 71\n",
      "iter 43445, ep 136, score 93.657869, steps 65\n",
      "iter 43531, ep 137, score 92.299140, steps 86\n",
      "iter 43597, ep 138, score 93.641797, steps 66\n",
      "iter 43662, ep 139, score 93.683167, steps 65\n",
      "iter 43743, ep 140, score 93.438447, steps 81\n",
      "iter 43809, ep 141, score 93.611080, steps 66\n",
      "iter 43874, ep 142, score 93.664508, steps 65\n",
      "iter 43942, ep 143, score 93.549058, steps 68\n",
      "iter 44008, ep 144, score 93.622160, steps 66\n",
      "iter 44073, ep 145, score 93.656425, steps 65\n",
      "iter 44138, ep 146, score 93.673692, steps 65\n",
      "iter 44204, ep 147, score 93.635087, steps 66\n",
      "iter 44301, ep 148, score 91.656801, steps 97\n",
      "iter 44368, ep 149, score 93.572884, steps 67\n",
      "iter 44434, ep 150, score 93.599984, steps 66\n",
      "iter 44499, ep 151, score 93.671498, steps 65\n",
      "iter 44564, ep 152, score 93.672448, steps 65\n",
      "iter 44631, ep 153, score 93.563558, steps 67\n",
      "iter 44726, ep 154, score 91.479624, steps 95\n",
      "iter 44791, ep 155, score 93.668465, steps 65\n",
      "iter 44856, ep 156, score 93.668898, steps 65\n",
      "iter 44921, ep 157, score 93.654076, steps 65\n",
      "iter 44988, ep 158, score 93.559129, steps 67\n",
      "iter 45053, ep 159, score 93.662076, steps 65\n",
      "iter 45128, ep 160, score 93.459317, steps 75\n",
      "iter 45193, ep 161, score 93.648433, steps 65\n",
      "iter 45258, ep 162, score 93.650194, steps 65\n",
      "iter 45324, ep 163, score 93.586031, steps 66\n",
      "iter 45389, ep 164, score 93.643323, steps 65\n",
      "iter 45455, ep 165, score 93.605231, steps 66\n",
      "iter 45522, ep 166, score 93.527247, steps 67\n",
      "iter 45592, ep 167, score 93.562021, steps 70\n",
      "iter 45659, ep 168, score 93.544799, steps 67\n",
      "iter 45726, ep 169, score 93.528387, steps 67\n",
      "iter 45793, ep 170, score 93.544919, steps 67\n",
      "iter 45860, ep 171, score 93.532902, steps 67\n",
      "iter 45927, ep 172, score 93.545647, steps 67\n",
      "iter 45994, ep 173, score 93.458183, steps 67\n",
      "iter 46061, ep 174, score 93.438279, steps 67\n",
      "iter 46128, ep 175, score 93.464301, steps 67\n",
      "iter 46195, ep 176, score 93.507404, steps 67\n",
      "iter 46262, ep 177, score 93.439179, steps 67\n",
      "iter 46329, ep 178, score 93.486629, steps 67\n",
      "iter 46397, ep 179, score 93.448294, steps 68\n",
      "iter 46464, ep 180, score 93.465806, steps 67\n",
      "iter 46531, ep 181, score 93.452436, steps 67\n",
      "iter 46598, ep 182, score 93.476762, steps 67\n",
      "iter 46665, ep 183, score 93.457539, steps 67\n",
      "iter 46732, ep 184, score 93.503596, steps 67\n",
      "iter 46800, ep 185, score 93.333887, steps 68\n",
      "iter 46867, ep 186, score 93.435041, steps 67\n",
      "iter 46933, ep 187, score 93.528608, steps 66\n",
      "iter 46999, ep 188, score 93.544496, steps 66\n",
      "iter 47065, ep 189, score 93.591187, steps 66\n",
      "iter 47136, ep 190, score 93.541665, steps 71\n",
      "iter 47202, ep 191, score 93.583979, steps 66\n",
      "iter 47274, ep 192, score 93.592861, steps 72\n",
      "iter 47343, ep 193, score 93.404450, steps 69\n",
      "iter 47408, ep 194, score 93.641312, steps 65\n",
      "iter 47503, ep 195, score 90.889741, steps 95\n",
      "iter 47568, ep 196, score 93.641167, steps 65\n",
      "iter 47635, ep 197, score 93.505963, steps 67\n",
      "iter 47706, ep 198, score 93.425933, steps 71\n",
      "iter 47797, ep 199, score 91.760404, steps 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-26 19:47:55,111] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/winter/Google Drive/handson-ml/tmp')\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "max_episode = 200\n",
    "gamma = 0.99\n",
    "tau = 0.001\n",
    "memory_size = 10000\n",
    "batch_size = 256\n",
    "memory_warmup = batch_size*3\n",
    "max_explore_eps = 100\n",
    "save_path = 'DDPG_net_Class.ckpt'\n",
    "\n",
    "tf.reset_default_graph()\n",
    "actorAsync = AsyncNets('Actor')\n",
    "actor,actor_target = actorAsync.get_subnets()\n",
    "criticAsync = AsyncNets('Critic')\n",
    "critic,critic_target = criticAsync.get_subnets()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    actorAsync.set_session(sess)\n",
    "    criticAsync.set_session(sess)\n",
    "    env = gym.make('MountainCarContinuous-v0')\n",
    "    env = wrappers.Monitor(env,'./tmp/',force=True)\n",
    "    obs = env.reset()\n",
    "    iteration = 0\n",
    "    episode = 0\n",
    "    episode_score = 0\n",
    "    episode_steps = 0\n",
    "    noise = UONoise()\n",
    "    memory = Memory(memory_size)\n",
    "    while episode < max_episode:\n",
    "        print('\\riter {}, ep {}'.format(iteration,episode),end='')\n",
    "        action = actor.predict_action(np.reshape(obs,[1,-1]))[0]\n",
    "        if episode<max_explore_eps: # exploration policy\n",
    "            p = episode/max_explore_eps\n",
    "            action = action*p + (1-p)*next(noise)\n",
    "        next_obs, reward, done,info = env.step(action)\n",
    "        memory.append([obs,action,reward,next_obs,done])\n",
    "        if iteration >= memory_warmup:\n",
    "            memory_batch = memory.sample_batch(batch_size)\n",
    "            extract_mem = lambda k : np.array([item[k] for item in memory_batch])\n",
    "            obs_batch = extract_mem(0)\n",
    "            action_batch = extract_mem(1)\n",
    "            reward_batch = extract_mem(2)\n",
    "            next_obs_batch = extract_mem(3)\n",
    "            done_batch = extract_mem(4)\n",
    "            action_next = actor_target.predict_action(next_obs_batch)\n",
    "            Q_next = critic_target.predict_Q(next_obs_batch,action_next)[:,0]\n",
    "            Qexpected_batch = reward_batch + gamma*(1-done_batch)*Q_next # target Q value\n",
    "            Qexpected_batch = np.reshape(Qexpected_batch,[-1,1])\n",
    "            # train critic\n",
    "            critic.train(obs_batch,action_batch,Qexpected_batch)\n",
    "            # train actor\n",
    "            action_grads = critic.compute_action_grads(obs_batch,action_batch)\n",
    "            actor.train(obs_batch,action_grads)\n",
    "            # async update\n",
    "            actorAsync.async_update(tau)\n",
    "            criticAsync.async_update(tau)\n",
    "        episode_score += reward\n",
    "        episode_steps += 1\n",
    "        iteration += 1\n",
    "        if done:\n",
    "            print(', score {:8f}, steps {}'.format(episode_score,episode_steps))\n",
    "#             if episode%5 == 0:\n",
    "                \n",
    "#                 Q_check = \n",
    "            obs = env.reset()\n",
    "            episode += 1\n",
    "            episode_score = 0\n",
    "            episode_steps = 0\n",
    "            noise = UONoise()\n",
    "            if episode%25==0:\n",
    "                saver.save(sess,save_path)\n",
    "        else:\n",
    "            obs = next_obs\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
